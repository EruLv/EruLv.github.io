<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>矩阵求导运算</title>
    <link href="/EruLv.github.io/2020/03/18/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E8%BF%90%E7%AE%97/"/>
    <url>/EruLv.github.io/2020/03/18/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E8%BF%90%E7%AE%97/</url>
    
    <content type="html"><![CDATA[<h2 id="矩阵求导运算推导与总结"><a href="#矩阵求导运算推导与总结" class="headerlink" title="矩阵求导运算推导与总结"></a>矩阵求导运算推导与总结</h2><h3 id="1-Gradient-Vector-in-scalar-out"><a href="#1-Gradient-Vector-in-scalar-out" class="headerlink" title="1.Gradient: Vector in, scalar out"></a>1.Gradient: Vector in, scalar out</h3><p>输入向量，输出标量。即映射关系为$f:R^N➡R$ 。</p><p>设$y = f(\boldsymbol{x})$ ,$\boldsymbol{x}$是Nx1的矢量，y是标量。</p><p>则求导结果为:</p><script type="math/tex; mode=display">\frac{\partial y}{\partial \boldsymbol{x}}=\left(\frac{\partial y}{\partial x_{1}}, \frac{\partial y}{\partial x_{2}}, \ldots, \frac{\partial y}{\partial x_{N}}\right)</script><h3 id="2-Jacobian-Vector-in-Vector-out"><a href="#2-Jacobian-Vector-in-Vector-out" class="headerlink" title="2.Jacobian: Vector in, Vector out"></a>2.Jacobian: Vector in, Vector out</h3><p>输入向量，输出也是向量，则需要用到 Jacobian矩阵。</p><p>假设映射关系为$f:R^N➡R^M$ ,输入输出都为向量。$\boldsymbol{y} = f(\boldsymbol{x})$ ,则有:</p><script type="math/tex; mode=display">\frac{\partial y}{\partial x}=\left(\begin{array}{ccc}\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{N}} \\\vdots & \ddots & \vdots \\\frac{\partial y_{M}}{\partial x_{1}} & \cdots & \frac{\partial y_{M}}{\partial x_{N}}\end{array}\right)</script><p>输出MxN的矩阵。</p><p>假如$W$ 是一个MxN的矩阵，$\boldsymbol{z}=\boldsymbol{W}\boldsymbol{x}$ ，$\boldsymbol{x}$为Nx1的列向量，$\boldsymbol{z}$为Mx1的列向量，该式的Jacobian矩阵为NxM维。</p><script type="math/tex; mode=display">z_{i}=\sum_{k=1}^{m} W_{i k} x_{k}</script><script type="math/tex; mode=display">\left(\frac{\partial \boldsymbol{z}}{\partial \boldsymbol{x}}\right)_{i j}=\frac{\partial z_{i}}{\partial x_{j}}=\frac{\partial}{\partial x_{j}} \sum_{k=1}^{m} W_{i k} x_{k}=\sum_{k=1}^{m} W_{i k} \frac{\partial}{\partial x_{j}} x_{k}=W_{i j}</script><p>所以</p><script type="math/tex; mode=display">\frac{\partial{\boldsymbol{z}}}{\partial{\boldsymbol{x}}} = \boldsymbol{W}</script>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>线性代数</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
